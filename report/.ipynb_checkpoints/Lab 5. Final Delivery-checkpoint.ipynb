{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">Lab 5: Neural Networks NERC</p>\n",
    "#### <p style=\"text-align: center;\"> Alejandra López de Aberasturi Gómez, Thèo Ding</p>\n",
    "\n",
    "\n",
    "### Introduction\n",
    "----------------\n",
    "\n",
    "The purpose of this assignment was the development of a neural network that would be able to succesfully perform named entity recognition tasks in a specialized domain such as the corresponding to the given dataset. \n",
    "The training dataset used for this design was that one comprised of all the sentences available at the /Train folder. Meanwhile, devel and test datasets corresponded respectively to the contents of /Devel and /Test folders. \n",
    "\n",
    "In all the tested implementations, the basic architecture was that one of a bidirectional LSTM (biLSTM hereafter). This kind of neural network is specially suited for our goals, for we have access to both the past and future input features for a given time. In such model, words need to be assigned to numerical (dense) vectors, and during training the neural network is able to learn improved representations for them. Performance might be increased by assigning these numerical-word vectors to pre-trained word embeddings learned with GloVe. Further, it is possible to input character-level embeddings so that the system can automatically capture prefixes, suffixes and other grammatical features of words, providing the classifier with richer representations. \n",
    "\n",
    "In our search for the most satistying implementation, we have experimented with both word and character-level embeddings. The former was always present in our networks, although we have tested the resulting accuracy when using both random and pre-trained word embeddings. The second one was optional and used randomized embedding. For the pre-trained case, we have tried all the vectors available in [this file](http://nlp.stanford.edu/data/glove.6B.zip), as well as the ones [here](http://nlp.stanford.edu/data/glove.840B.300d.zip). We present the results regarding the second package.\n",
    "\n",
    "Once the random embeddings were set-up, the layers were concatenated and connected to the biLSTM network. In the case of the character-level embedding, it was first connected to a smaller biLSTM, and it was this layer that was concatenated with the rest of embeddings. \n",
    "\n",
    "#### Feature Augmentation and other embeddings\n",
    "\n",
    "We have also worked with several experimental arrangements that used **lowercased** and non-lowercased word embeddings (*i.e.* different embedding layers for lowercased words and non-lowercased words), as well as **PoS-tag** embeddings. Just as in the case of the character-level embedding, these two were random (*vs.* pre-trained). \n",
    "Finally, some features (**prefixes, words containing dashes, keywords**, etc.) were augmented (extracted and input to a feature embedding) in order to boost the performance of the network regarding group-type entities, whose recall and precision scores tended to be low. Again, keras.layers. Embedding layer was used for this processing. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and hyper-parameters\n",
    "------------------------------------\n",
    "The trained models have all been tested blindly on unseen test data. The split of the dataset into train and evaluation was, as aknowledged before, straightforward: we worked on the /Train folder when training the model and tested on the /Devel and /Test datasets. During training, the validation dataset's loss and viterbi accuracy were monitored. In fact, the first of these metrics was used as an early-stopping criterion. The hyper-parameters of the LSTM include: \n",
    "\n",
    "* Number of hidden nodes for both biLSTMs and dense layer: \\\\( \\left(H_{w}, H_{c}, H_{d}\\right)\\in\\{25,50,100\\} \\\\)\n",
    "* Word embedding dimension, \\\\(d_{w}\\in\\{50,100,200,300\\}\\\\)\n",
    "* Character embedding dimension, \\\\(d_{c}\\in\\{25,50\\}\\\\)\n",
    "* Learning rate was left at its default value (*i.e.* 0.01)\n",
    "* Batch size, which was 32 for all the experiments.\n",
    "* Number of epochs, which was set to 15 with early stopping when the validation started decreasing consecutively after 2 epochs. \n",
    "\n",
    "The best model from the validation set was tested finally on the unseen test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performed Experiments\n",
    "----------------------------------\n",
    "In total, more than 30 experiments were carried out in our pursuit of the best possible architecture. As we have already mentioned, the different variants differed in the value of the hyper-parameters (always ranging among the values refered above). Furthermore, we tried multiple combinations of the embedding layers. For instance, just to name a few: \n",
    "\n",
    "* Randomized word embedding + char embedding\n",
    "* Randomized word embedding + pos-tag embedding + char embedding \n",
    "* Randomized word embedding + feature augmentation + char embedding \n",
    "* Randomized word embedding + pos-tag embedding + feature augmentation + char embedding\n",
    "* Lowercased/non-lowercased randomized word embedding + char embedding\n",
    "* Lowercased/non-lowercased randomized word embedding + pos-tag embedding + char embedding\n",
    "* Lowercased/non-lowercased randomized word embedding + feature augmentation + char embedding\n",
    "* Lowercased/non-lowercased randomized word embedding + pos-tag embedding + feature augmentation + char embedding\n",
    "* All the previous configurations without char embedding\n",
    "\n",
    "\n",
    "Regarding the main LSTM layer, we have tested the performance of the system when is fed with pre-trained and random word vectors. As mentioned in the Introduction section, such pre-trained vectors were from the Wikipedia + Gigaword 5 and Common Crawl (840B tokens, 2.2M vocab) datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "--------------------\n",
    "\n",
    "In what follows, we will focus on the architecture of the experimental arrangement that proved to perform best. More than 30 slight variations of this model have been tested until we have finally arrived to this one, with F1 scores varying from 0.3 to the one presented here.\n",
    "Nevertheless, the general structure doesn't change, and further feature-augmenting embeddings simply involve the addition of random embedding layers. \n",
    "It is noteworthy, however, that the fully-connected layer after the main biLSTM was not always present, as well as the dropout layer introduced to prevent the model from overfitting. The table below shows all the hyper parameters used for the experiments reported in the Results section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hyper-paramenter | Value |\n",
    "|--- | --- |\n",
    "|Word embedding dim, \\\\(d_{w}\\\\) | 300 (common crawl)|\n",
    "|Character embedding dim, \\\\(d_{c}\\\\) | 25 (random) |\n",
    "|Character biLSTM hidden layer dimension, \\\\(H_{c}\\\\) | 25|\n",
    "|Main biLSTM hidden layer dimension, \\\\(H_{w}\\\\)|80|\n",
    "|Dense hidden layer dimension, \\\\(H_{d}\\\\) | 160|\n",
    "|Activation in dense hidden layer| tanh |\n",
    "|Dropout  | 0.2|\n",
    "|Recurrent dropout | 0.5\n",
    "|Optimizer | adam |\n",
    "|Loss | crf_loss|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "--------------\n",
    "Below we present the results obtained when using the configuration described. The final viterbi accuracy reached a value of 0.97 in the validation dataset, the F1 score being of 0.64 (P = 0.67, R = 0.63). We can see that the results regarding the *drug_n* entity type and the *group* type are not as good as the rest, which was to be expected given the low presence (if any) of examples of this class in the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "----------\n",
    "\n",
    "We present here the code of build_network. The `config` parameter refers to an object containing all the hyper-parameters of the experimental arrangement. Such parameters were introduced by means of a `.json` file so that several experiments could be launched automatically from a `bash` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(idx, config):\n",
    "    '''\n",
    "    Builds the nn. Receives the index dictionary with the encondings \n",
    "    of words and tags , and the maximum length of sentences\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    idx: dict\n",
    "\n",
    "    config: NetConfig instance\n",
    "        Contains configuration of the neural network\n",
    "        \n",
    "    Returns:\n",
    "    --------    \n",
    "    model: neural network\n",
    "    '''\n",
    "\n",
    "    # sizes\n",
    "\n",
    "    n_pos =             len(idx['pos'])             # UNK & PAD considered\n",
    "    n_case =            len(idx['case'])            # PAD considered\n",
    "    n_type =            len(idx['type'])            # PAD considered\n",
    "    n_chars =           len(idx['chars'])           # UNK & PAD considered\n",
    "    n_words =           len(idx['words'])           # UNK & PAD considered\n",
    "    n_tags =            len(idx['tags'])            # PAD considered\n",
    "    max_len_sentences = idx['max_len_sentences']   \n",
    "    max_len_words =     idx['max_len_words'] \n",
    "\n",
    "    # ************************************************\n",
    "\n",
    "    # architectural parameters\n",
    "\n",
    "    pre_trained =       config.pre_trained\n",
    "    w_embedding =       config.w_embedding\n",
    "    c_embedding =       config.c_embedding\n",
    "    lstm_char_units =   config.lstm_char_units\n",
    "    lstm_main_units =   config.lstm_main_units\n",
    "    dense_units =       config.dense_units\n",
    "    return_sequences =  config.return_sequences\n",
    "    mask_zero =         config.mask_zero\n",
    "    activation =        config.activation\n",
    "\n",
    "    #training parameters\n",
    "\n",
    "    dropout =       config.dropout\n",
    "    rcrr_dropout =  config.rcrr_dropout\n",
    "    optimizer =     config.optimizer\n",
    "    loss =          config.loss\n",
    "    metrics =       config.metrics\n",
    "\n",
    "    #********************************************************\n",
    "\n",
    "    # create network layers\n",
    "\n",
    "    # type embedding\n",
    "    #---------------#\n",
    "\n",
    "    type_inp = Input(shape=(max_len_sentences,))\n",
    "    type_emb = Embedding(\n",
    "        input_dim=n_type,\n",
    "        output_dim=w_embedding,\n",
    "        input_length=max_len_sentences,\n",
    "        mask_zero=mask_zero)(type_inp)\n",
    "\n",
    "    # pos embedding\n",
    "    #--------------#\n",
    "\n",
    "    pos_inp = Input(shape=(max_len_sentences,))\n",
    "    pos_emb = Embedding(\n",
    "        input_dim=n_pos,\n",
    "        output_dim=w_embedding,\n",
    "        input_length=max_len_sentences,\n",
    "        mask_zero=mask_zero)(pos_inp)\n",
    "\n",
    "    # capitalization words embedding\n",
    "    #--------------------------#    \n",
    "\n",
    "    case_inp = Input(shape=(max_len_sentences,))\n",
    "    case_emb = Embedding(\n",
    "        input_dim=n_case,\n",
    "        output_dim=w_embedding,\n",
    "        input_length=max_len_sentences,\n",
    "        mask_zero=mask_zero)(case_inp)\n",
    "\n",
    "    # word embedding\n",
    "    # --------------#\n",
    "\n",
    "    word_inp = Input(shape=(max_len_sentences,))\n",
    "      \n",
    "    if pre_trained: \n",
    "\n",
    "        #  word embedding option (1): load pre-trained embeddings \n",
    "        # and create the customized weights matrix according to our dataset\n",
    "\n",
    "        word_emb = Embedding(\n",
    "            input_dim=n_words, \n",
    "            output_dim=w_embedding,\n",
    "            weights=[embedding_matrix(idx, n_words, w_embedding)], \n",
    "            trainable=False)(word_inp)\n",
    "\n",
    "    else:\n",
    "\n",
    "        # word embedding option (2): random embedding\n",
    "\n",
    "        word_emb = Embedding(\n",
    "        input_dim=n_words, \n",
    "        output_dim=w_embedding,\n",
    "        input_length=max_len_sentences, \n",
    "        mask_zero=mask_zero)(word_inp)        \n",
    "\n",
    "\n",
    "    #char embedding + char biLSTM\n",
    "    #----------------------------\n",
    "\n",
    "    char_inp = Input(shape=(max_len_sentences, max_len_words)) \n",
    "\n",
    "    char_emb = TimeDistributed(\n",
    "                    Embedding(\n",
    "                        input_dim=n_chars,\n",
    "                        output_dim=c_embedding,\n",
    "                        input_length=max_len_words,\n",
    "                        mask_zero=mask_zero)\n",
    "                    )(char_inp)  \n",
    "\n",
    "    char_biLSTM = TimeDistributed(\n",
    "                    Bidirectional(LSTM(\n",
    "                    units=lstm_char_units, \n",
    "                    return_sequences=False,\n",
    "                    recurrent_dropout=rcrr_dropout, \n",
    "                    dropout=dropout))\n",
    "                    )(char_emb) \n",
    "    \n",
    "    # main LSTM\n",
    "    #---------#\n",
    "\n",
    "    model = concatenate([\n",
    "        word_emb, \n",
    "        char_biLSTM,\n",
    "        case_emb,\n",
    "        # pos_emb, \n",
    "        # type_emb\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    # model = Dropout(dropout)(model)\n",
    "\n",
    "    model = Bidirectional(LSTM(units=lstm_main_units, return_sequences=return_sequences,\n",
    "                recurrent_dropout=rcrr_dropout, dropout=dropout))(model)\n",
    "\n",
    "    model = TimeDistributed(Dense(units=dense_units, activation=activation))(model) \n",
    "\n",
    "    model = TimeDistributed(Dense(units=n_tags, activation=activation))(model) \n",
    "    \n",
    "    # model = Dropout(dropout)(model)\n",
    "\n",
    "    # CRF layer\n",
    "    #----------\n",
    "\n",
    "    crf = CRF(n_tags)\n",
    "\n",
    "    out = crf(model)               \n",
    "    \n",
    "    # create and compile model\n",
    "\n",
    "    model = Model([\n",
    "        word_inp, \n",
    "        char_inp,\n",
    "        case_inp,\n",
    "        # pos_inp, \n",
    "        # type_inp, \n",
    "        \n",
    "        ], out)\n",
    "\n",
    "    \n",
    "    if str.lower(optimizer) == 'nadam': \n",
    "        optimizer = Nadam()\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=crf_loss, metrics=[crf_accuracy])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "------------------\n",
    "### [1. LSTM-CRF  for Drug-Named Entity Recognition.](https://www.mdpi.com/1099-4300/19/6/283)\n",
    "(Donghuo Zeng, Chengjie Sun, Lei Lin and Bingquan Liu)\n",
    "\n",
    "### [2. Recurrent Neural Networks with Specialized Word Embeddings for health-domain named-entity recognition](https://arxiv.org/abs/1706.09569)\n",
    "(Iñigo Jauregi Unanue, Ehsan Zare Borzeshi and Massimo Picardi)\n",
    "\n",
    "### [3. End-to-end Sequence Labelling via Bi-directional LSTM-CNNs-CRF](https://arxiv.org/abs/1603.01354)\n",
    "(Xuezhe Ma and Eduard Hovy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
